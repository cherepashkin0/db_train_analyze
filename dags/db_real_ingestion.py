# db_real_ingestion.py

import asyncio
import os
import json
import clickhouse_connect
from api_client import fetch_and_save
from iris_parser import parse_db_xml
from airflow.providers.postgres.hooks.postgres import PostgresHook

# --- –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –ö–û–ù–§–ò–ì–ê ---
def load_config():
    # –ú—ã –∑–Ω–∞–µ–º, —á—Ç–æ –≤ Docker Airflow –ø–∞–ø–∫–∞ dags –≤—Å–µ–≥–¥–∞ —Ç—É—Ç:
    base_dir = "/opt/airflow/dags"
    config_path = os.path.join(base_dir, "config", "railway_config.json")
    
    print(f"üîç –ò—â—É –∫–æ–Ω—Ñ–∏–≥ –∑–¥–µ—Å—å: {config_path}")

    # === –û–¢–õ–ê–î–ö–ê (DEBUG) ===
    # –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –≤–∏–¥–∏—Ç Docker
    try:
        config_dir = os.path.join(base_dir, "config")
        if os.path.exists(config_dir):
            print(f"üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ {config_dir}: {os.listdir(config_dir)}")
        else:
            print(f"‚ùå –ü–∞–ø–∫–∞ {config_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            print(f"üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–æ—Ä–Ω—è {base_dir}: {os.listdir(base_dir)}")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ –ø—É—Ç–µ–π: {e}")
    # =======================

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                print("‚úÖ –ö–æ–Ω—Ñ–∏–≥ —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç.")
                return json.load(f)
        except Exception as e:
            print(f"‚ùå –§–∞–π–ª –µ—Å—Ç—å, –Ω–æ –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {e}")
    else:
        print("‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ —ç—Ç–æ–º—É –ø—É—Ç–∏.")

    print("‚ö† –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (Berlin Hbf).")
    return {
        "stations": {"8011160": "Berlin Hbf"}, 
        "monitored_types": []
    }

# --- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í POSTGRES ---
def log_ingestion_status(context, status, records_count, error_message=None):
    try:
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        create_sql = """
        CREATE TABLE IF NOT EXISTS api_ingestion_log (
            run_id SERIAL PRIMARY KEY,
            dag_id VARCHAR(50),
            execution_date VARCHAR(50),
            status VARCHAR(20),
            records_count INT,
            error_message TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        );
        """
        pg_hook.run(create_sql)

        insert_sql = """
            INSERT INTO api_ingestion_log (dag_id, execution_date, status, records_count, error_message)
            VALUES (%s, %s, %s, %s, %s);
        """
        
        dag_id = str(context['dag'].dag_id)
        execution_date = str(context['execution_date'])
        
        pg_hook.run(insert_sql, parameters=(dag_id, execution_date, status, records_count, error_message))
        print(f"üìù –°—Ç–∞—Ç—É—Å '{status}' –∑–∞–ø–∏—Å–∞–Ω –≤ Postgres.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞ –≤ Postgres: {e}")

# --- –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
async def run_real_ingestion(context):
    config = load_config()
    stations = config.get("stations", {})
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–∏–ø—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ set
    target_types = set(filter(None, config.get("monitored_types", [])))
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è –∏–ª–∏ –ø—É—Å—Ç–æ–π, stations –±—É–¥–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º
    queries = [
        {"url": f"https://apis.deutschebahn.com/db-api-marketplace/apis/timetables/v1/fchg/{eva}"}
        for eva in stations.keys()
    ]
    
    print(f"üåç –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(stations)} —Å—Ç–∞–Ω—Ü–∏–π...")

    output_path = "/opt/airflow/data/raw_api_data"
    
    df = await fetch_and_save(
        queries=queries,
        output_path=output_path,
        max_concurrent=3,
        rate_limit=60
    )

    # === –ü–†–û–í–ï–†–ö–ê –ù–ê –û–®–ò–ë–ö–ò ===
    failed_requests = df['error'].notna().sum()
    total_requests = len(queries)
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ API: {total_requests - failed_requests}/{total_requests} —É—Å–ø–µ—à–Ω—ã—Ö.")

    if failed_requests == total_requests and total_requests > 0:
        error_msg = f"CRITICAL: All {total_requests} API requests failed."
        log_ingestion_status(context, 'FAILED', 0, error_msg)
        raise Exception(error_msg)

    # === –ó–ê–ì–†–£–ó–ö–ê –í CLICKHOUSE ===
    client = clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )

    all_parsed_data = []
    
    for _, row in df.iterrows():
        if row['error']: continue

        eva_id = row['url'].split('/')[-1]
        city = stations.get(eva_id, "Unknown")
        
        if row['response_data']:
            parsed_rows = parse_db_xml(row['response_data'], city)
            
            if target_types:
                parsed_rows = [r for r in parsed_rows if r[2] in target_types]
                
            all_parsed_data.extend(parsed_rows)

    count = len(all_parsed_data)

    if all_parsed_data:
        client.insert('train_delays', all_parsed_data, 
                        column_names=[
                            'timestamp', 'city', 'train_type', 'train_id', 
                            'planned_departure', 'actual_departure', 
                            'delay_in_min', 'is_cancelled',
                            'origin', 'destination'
                        ])
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫ –≤ ClickHouse.")
    else:
        print("‚ö† API –æ—Ç–≤–µ—Ç–∏–ª —É—Å–ø–µ—à–Ω–æ, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç (–∏–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã).")

    log_ingestion_status(context, 'SUCCESS', count)

def main(**kwargs):
    asyncio.run(run_real_ingestion(kwargs))

