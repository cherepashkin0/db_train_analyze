import asyncio
import os
import json
import clickhouse_connect
from datetime import datetime, timedelta
from api_client import fetch_and_save
from iris_parser import parse_db_xml
from airflow.providers.postgres.hooks.postgres import PostgresHook

# --- ÐšÐžÐÐ¤Ð˜Ð“Ð£Ð ÐÐ¦Ð˜Ð¯ ---
def load_config():
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¹."""
    base_dir = "/opt/airflow/dags"
    config_path = os.path.join(base_dir, "config", "railway_config.json")
    
    print(f"ðŸ” Ð˜Ñ‰Ñƒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ Ð·Ð´ÐµÑÑŒ: {config_path}")
    
    # ÐžÑ‚Ð»Ð°Ð´ÐºÐ° Ð¿ÑƒÑ‚ÐµÐ¹
    try:
        config_dir = os.path.join(base_dir, "config")
        if os.path.exists(config_dir):
            print(f"ðŸ“‚ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¿Ð°Ð¿ÐºÐ¸ {config_dir}: {os.listdir(config_dir)}")
    except: pass

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                print(f"âœ… ÐšÐ¾Ð½Ñ„Ð¸Ð³ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {len(config.get('stations', {}))} ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¹")
                return config
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ JSON: {e}")
    
    return {
        "stations": {"8011160": "Berlin Hbf"}, 
        "monitored_types": []
    }

# --- HELPER: CLICKHOUSE CLIENT ---
def get_ch_client():
    return clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )

# --- Ð›ÐžÐ“Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• ---
def log_status(context, stage, status, msg=""):
    """ÐŸÐ¸ÑˆÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÑ‚Ð°Ð¿Ð° Ð² ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ Ð¸ Ð² Postgres."""
    print(f"[{stage}] {status}: {msg}")
    
    try:
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        sql = """
            INSERT INTO api_ingestion_log (dag_id, execution_date, status, error_message)
            VALUES (%s, %s, %s, %s)
        """
        dag_id = str(context['dag'].dag_id)
        execution_date = str(context.get('execution_date', datetime.now()))
        
        pg_hook.run("""
            CREATE TABLE IF NOT EXISTS api_ingestion_log (
                run_id SERIAL PRIMARY KEY,
                dag_id VARCHAR(50),
                execution_date VARCHAR(50),
                status VARCHAR(20),
                error_message TEXT,
                created_at TIMESTAMP DEFAULT NOW()
            );
        """)
        
        pg_hook.run(sql, parameters=(dag_id, execution_date, status, f"{stage}: {msg}"))
    except Exception as e:
        print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð»Ð¾Ð³Ð° Ð² Postgres: {e}")

# ==========================================
# 1. EXTRACT DATA (API -> Parquet/Bronze)
# ==========================================
async def extract_data(config):
    stations = config.get("stations", {})
    queries = []
    for eva_id in stations.keys():
        queries.append({"url": f"https://apis.deutschebahn.com/db-api-marketplace/apis/timetables/v1/fchg/{eva_id}"})
    
    print(f"ðŸŒ TASK 1: EXTRACT. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ {len(stations)} ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¹...")
    return await fetch_and_save(
        queries=queries,
        output_path="/opt/airflow/data/raw_api_data",
        max_concurrent=5,
        rate_limit=60
    )

# ==========================================
# 2. LOAD TO SILVER (Parquet -> ClickHouse Raw)
# ==========================================
def load_to_silver(df, config):
    print("ðŸ“¥ TASK 2: LOAD TO SILVER...")
    stations = config.get("stations", {})
    target_types = set(config.get("monitored_types", []))
    
    all_parsed = []
    for _, row in df.iterrows():
        if row['error']: continue
        eva_id = row['url'].split('/')[-1]
        city = stations.get(eva_id, "Unknown")
        
        if row['response_data']:
            rows = parse_db_xml(row['response_data'], city)
            if target_types:
                rows = [r for r in rows if r[2] in target_types]
            all_parsed.extend(rows)
            
    if not all_parsed:
        print("âš  LOAD: ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸.")
        return 0

    client = get_ch_client()
    
    client.insert('train_delays', all_parsed, 
                  column_names=['timestamp', 'city', 'train_type', 'train_id', 
                                'planned_departure', 'actual_departure', 
                                'delay_in_min', 'is_cancelled', 'origin', 'destination'])
    print(f"âœ… LOAD: Ð’ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {len(all_parsed)} ÑÑ‚Ñ€Ð¾Ðº Ð² Silver ÑÐ»Ð¾Ð¹ (train_delays).")
    return len(all_parsed)

# ==========================================
# 3. DATA QUALITY CHECK (Validation)
# ==========================================
def data_quality_check():
    print("ðŸ§ TASK 3: DATA QUALITY CHECK...")
    client = get_ch_client()
    
    # === ÐÐžÐ’Ð«Ð• Ð¢Ð•Ð¡Ð¢Ð« ===
    checks = [
        # 1. Validate for Nulls (ÐŸÑƒÑÑ‚Ñ‹Ðµ ID Ð¿Ð¾ÐµÐ·Ð´Ð¾Ð² Ð¸Ð»Ð¸ Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ð²)
        ("Null Check: Train IDs", 
         "SELECT count() FROM train_delays WHERE train_id = '' AND actual_departure > now() - INTERVAL 1 HOUR"),
         
        ("Null Check: Cities", 
         "SELECT count() FROM train_delays WHERE city = '' AND actual_departure > now() - INTERVAL 1 HOUR"),

        # 2. Test Range Constraints (Ð—Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð°Ð´ÐµÐºÐ²Ð°Ñ‚Ð½Ð¾Ð¹)
        ("Range Check: Negative Delays", 
         "SELECT count() FROM train_delays WHERE delay_in_min < 0"),
         
        ("Range Check: Extreme Delays (>1000 min)", 
         "SELECT count() FROM train_delays WHERE delay_in_min > 1000 AND actual_departure > now() - INTERVAL 1 HOUR"),
         
        ("Range Check: Future Data (>2 Days)", 
         "SELECT count() FROM train_delays WHERE actual_departure > now() + INTERVAL 2 DAY"),

        # 3. Verify Referential Integrity (ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð³Ð¾Ñ€Ð¾Ð´ Ð¸Ð·Ð²ÐµÑÑ‚ÐµÐ½)
        ("Ref Integrity: Unknown Stations", 
         "SELECT count() FROM train_delays WHERE city = 'Unknown' AND actual_departure > now() - INTERVAL 1 HOUR")
    ]
    
    failed_checks = []
    
    for check_name, sql in checks:
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (Ñ‡Ð¸ÑÐ»Ð¾ ÑÑ‚Ñ€Ð¾Ðº, Ð½Ð°Ñ€ÑƒÑˆÐ°ÑŽÑ‰Ð¸Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾)
            result = client.query(sql).result_rows[0][0]
            
            if result > 0:
                msg = f"âŒ DQ FAIL: {check_name} -> Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {result} Ð¿Ð»Ð¾Ñ…Ð¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                print(msg)
                # Ð”Ð»Ñ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¾Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð½Ðµ Ñ€Ð¾Ð½ÑÑ‚ÑŒ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½, Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð°Ð»ÐµÑ€Ñ‚Ð¸Ñ‚ÑŒ
                # ÐÐ¾ Ð´Ð»Ñ ÑÑ‚Ñ€Ð¾Ð³Ð¸Ñ… Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ð¹ ÑÐ¿Ñ€Ð¸Ð½Ñ‚Ð° - Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾ÑˆÐ¸Ð±Ð¾Ðº
                failed_checks.append(msg)
            else:
                print(f"âœ… DQ PASS: {check_name}")
                
        except Exception as e:
            print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ {check_name}: {e}")
            failed_checks.append(f"SQL Error in {check_name}: {e}")
            
    if failed_checks:
        # Ð Ð¾Ð½ÑÐµÐ¼ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
        raise Exception(f"Data Quality Checks Failed:\n" + "\n".join(failed_checks))

# ==========================================
# 4. TRANSFORM GOLD (Silver -> Aggregated)
# ==========================================
def transform_gold():
    print("ðŸ”¨ TASK 4: TRANSFORM GOLD...")
    client = get_ch_client()
    
    query = """
    INSERT INTO daily_train_stats
    SELECT
        toDate(actual_departure) as stat_date,
        city,
        train_type,
        count() as total_trains,
        countIf(delay_in_min > 0) as delayed_trains,
        avgIf(delay_in_min, delay_in_min > 0) as avg_delay,
        max(delay_in_min) as max_delay,
        now() as created_at
    FROM train_delays
    WHERE actual_departure >= toStartOfDay(now())
    GROUP BY stat_date, city, train_type
    """
    
    client.command("ALTER TABLE daily_train_stats DELETE WHERE stat_date = toDate(now())")
    client.command(query)
    print("âœ… TRANSFORM: Gold ÑÐ»Ð¾Ð¹ (daily_train_stats) Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½.")

# --- ORCHESTRATOR (Ð’Ð½ÑƒÑ‚Ñ€Ð¸ ÑÐºÑ€Ð¸Ð¿Ñ‚Ð°) ---
# ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: Ð§Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¸Ð´ÐµÑ‚ÑŒ ÑÑ‚Ð¸ ÑˆÐ°Ð³Ð¸ Ð³Ñ€Ð°Ñ„Ð¸Ñ‡ÐµÑÐºÐ¸ Ð² Airflow, 
# Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ extract_data, load_to_silver Ð¸ Ñ‚.Ð´. 
# Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ PythonOperator Ð² Ñ„Ð°Ð¹Ð»Ðµ DAG, Ð° Ð½Ðµ Ð·Ð´ÐµÑÑŒ.
# ÐÐ¾ Ð¿Ð¾ÐºÐ° Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð°Ðº Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÐºÐ¾Ð´Ð°.

async def run_pipeline(context):
    config = load_config()
    
    # 1. EXTRACT
    try:
        df = await extract_data(config)
        
        # Tech Check: API health
        failed_count = df['error'].notna().sum()
        if failed_count == len(df) and len(df) > 0:
            raise Exception("CRITICAL: Ð’ÑÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº API ÑƒÐ¿Ð°Ð»Ð¸.")
        if failed_count > 0:
            print(f"âš  WARNING: {failed_count}/{len(df)} Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹.")
    except Exception as e:
        log_status(context, "EXTRACT", "FAILED", str(e))
        raise

    # 2. LOAD
    try:
        count = load_to_silver(df, config)
    except Exception as e:
        log_status(context, "LOAD", "FAILED", str(e))
        raise

    # 3. DQ CHECK
    if count > 0:
        try:
            data_quality_check()
        except Exception as e:
            log_status(context, "DQ_CHECK", "FAILED", str(e))
            raise

        # 4. TRANSFORM
        try:
            transform_gold()
        except Exception as e:
            log_status(context, "TRANSFORM", "FAILED", str(e))
            raise
            
    log_status(context, "PIPELINE", "SUCCESS", f"Processed {count} records")

def main(**kwargs):
    asyncio.run(run_pipeline(kwargs))