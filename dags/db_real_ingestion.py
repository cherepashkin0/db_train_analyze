"""
Deutsche Bahn Train Delays Pipeline
====================================
Medallion Architecture: Bronze -> Silver -> Gold
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

import asyncio
import os
import json
import clickhouse_connect
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ parser —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç unique_id –ø–æ—Å–ª–µ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
from api_client import generate_plan_queries, generate_fchg_queries, fetch_and_save
from iris_parser import parse_plan_xml, parse_fchg_xml 

# =============================================================================
# CONFIGURATION
# =============================================================================

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

DAG_ID = 'db_train_medallion_pipeline'
BRONZE_PATH = "/opt/airflow/data/bronze/train_api"
CONFIG_PATH = "/opt/airflow/dags/config/railway_config.json"


def load_config():
    """Load station configuration."""
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"stations": {"8011160": "Berlin Hbf"}, "monitored_types": [], "hours_back": 12, "hours_forward": 12}


def get_ch_client():
    return clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )


def log_pipeline_stage(dag_id: str, stage: str, status: str, records_processed: int = 0, error_message: str = None):
    """Log pipeline stage to Postgres metadata table."""
    try:
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        pg_hook.run("""
            CREATE TABLE IF NOT EXISTS pipeline_runs (
                id SERIAL PRIMARY KEY,
                dag_id VARCHAR(100),
                stage VARCHAR(50),
                status VARCHAR(20),
                records_processed INTEGER,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT NOW()
            );
        """)
        pg_hook.run(
            """INSERT INTO pipeline_runs (dag_id, stage, status, records_processed, error_message) 
               VALUES (%s, %s, %s, %s, %s)""",
            parameters=(dag_id, stage, status, records_processed, error_message)
        )
        print(f"üìù Logged: {stage} -> {status} ({records_processed} records)")
    except Exception as e:
        print(f"‚ö† Failed to log to Postgres: {e}")


# =============================================================================
# BRONZE LAYER: Raw Data Extraction
# =============================================================================

def bronze_extract(**context):
    print("=" * 60)
    print("ü•â BRONZE LAYER: Starting raw data extraction")
    print("=" * 60)
    
    config = load_config()
    stations = config.get("stations", {})
    # –ö–∞—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ–ª–Ω—ã–µ —Å—É—Ç–∫–∏ (-12...+12), —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø–æ–ª–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É
    hours_back = config.get("hours_back", 12)
    hours_forward = config.get("hours_forward", 12)
    
    plan_queries = generate_plan_queries(stations, hours_back, hours_forward)
    fchg_queries = generate_fchg_queries(stations)
    all_queries = plan_queries + fchg_queries
    
    print(f"üì° API Queries: {len(all_queries)} total")
    
    async def run_fetch():
        return await fetch_and_save(
            queries=all_queries,
            output_path=BRONZE_PATH,
            max_concurrent=10,
            rate_limit=60,
        )
    
    df = asyncio.run(run_fetch())
    
    total = len(df)
    failed = df['error'].notna().sum()
    success = total - failed
    
    if total == 0:
        raise Exception("CRITICAL: No queries generated")
    
    if failed == total:
        raise Exception("CRITICAL: All API requests failed")
    
    log_pipeline_stage(DAG_ID, 'bronze', 'SUCCESS', success)
    
    # –ü–µ—Ä–µ–¥–∞–µ–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–∞—Å–∫—É
    context['ti'].xcom_push(key='bronze_path', value=BRONZE_PATH)
    return total


# =============================================================================
# SILVER LAYER: Parsing, Cleaning, Validation
# =============================================================================

def silver_transform(**context):
    print("=" * 60)
    print("ü•à SILVER LAYER: Starting transformation")
    print("=" * 60)
    
    import pandas as pd
    from pathlib import Path
    
    config = load_config()
    target_types = set(config.get("monitored_types", []))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã (–¥–æ–±–∞–≤–ª—è–µ–º unique_id)
    ensure_silver_tables()
    
    bronze_path = context['ti'].xcom_pull(key='bronze_path', task_ids='bronze_extract')
    bronze_path = Path(bronze_path or BRONZE_PATH)
    
    parquet_files = list(bronze_path.glob("**/*.parquet"))
    if not parquet_files:
        raise Exception("No bronze data found")
    
    # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
    dfs = [pd.read_parquet(f) for f in parquet_files]
    df = pd.concat(dfs, ignore_index=True)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –¥–Ω—è)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    cutoff = datetime.now() - timedelta(days=2)
    df = df[df['timestamp'] >= cutoff]
    
    print(f"üìä Bronze records loaded: {len(df)}")
    
    # --- PARSING & DEDUPLICATION ---
    print("\nüîÑ Parsing XML responses...")
    
    # –ö–ª—é—á —Å–ª–æ–≤–∞—Ä—è —Ç–µ–ø–µ—Ä—å: (unique_train_id, planned_departure, city)
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å –ø–æ–µ–∑–¥–∞ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º ID, –ø—Ä–æ—Ö–æ–¥—è—â–∏–µ —Å—Ç–∞–Ω—Ü–∏—é –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è
    trains_dict = {} 
    
    parse_errors = 0
    
    for _, row in df.iterrows():
        if row['error'] or not row['response_data']: continue
        
        station_name = row.get('station_name', 'Unknown')
        query_type = row.get('query_type', 'plan')
        
        try:
            # –ü–∞—Ä—Å–∏–º XML
            if query_type == 'fchg':
                rows = parse_fchg_xml(row['response_data'], station_name)
            else:
                rows = parse_plan_xml(row['response_data'], station_name)
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞–º
            if target_types:
                rows = [r for r in rows if r[2] in target_types]
            
            # --- –õ–û–ì–ò–ö–ê –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò ---
            for row_data in rows:
                # –í–ê–ñ–ù–û: –ú—ã –æ–∂–∏–¥–∞–µ–º, —á—Ç–æ parser —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç unique_id –≤ –∫–æ–Ω—Ü–µ –∫–æ—Ä—Ç–µ–∂–∞!
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ row_data: 
                # [0:timestamp, 1:city, 2:type, 3:human_id, 4:planned, 5:actual, 6:delay, 7:canc, 8:orig, 9:dest, 10:UNIQUE_ID]
                
                # –ï—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –µ—â–µ –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω –∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç unique_id, –∏—Å–ø–æ–ª—å–∑—É–µ–º human_id –∫–∞–∫ fallback (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ä–∞)
                if len(row_data) > 10:
                    unique_id = row_data[10]
                else:
                    unique_id = row_data[3] # Fallback –Ω–∞ "ICE 101"
                
                planned_dep = row_data[4]
                city = row_data[1]
                
                # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á. –ï—Å–ª–∏ –ø–æ–µ–∑–¥ –ø—Ä–æ—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω—Ü–∏—é 2 —Ä–∞–∑–∞ (–≤ 12:00 –∏ 16:00),
                # planned_dep –±—É–¥–µ—Ç —Ä–∞–∑–Ω—ã–º, –∏ –∑–∞–ø–∏—Å–∏ –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—à—É—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞.
                key = (unique_id, planned_dep, city)
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–∞–Ω–Ω—ã—Ö: FCHG (–∏–∑–º–µ–Ω–µ–Ω–∏—è) –≤–∞–∂–Ω–µ–µ PLAN (—Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è)
                if query_type == 'fchg':
                    trains_dict[key] = row_data
                elif key not in trains_dict:
                    trains_dict[key] = row_data
                    
        except Exception as e:
            parse_errors += 1
            if parse_errors <= 1: print(f"‚ö† Parse error: {e}")

    print(f" ¬† - Unique trains after merge: {len(trains_dict)}")
    
    if not trains_dict:
        raise Exception("No records after parsing")
    
    all_records = list(trains_dict.values())
    
    # --- DATA QUALITY & LOAD ---
    dq_results = run_silver_dq_checks(all_records)
    
    if dq_results['critical_failures']:
        raise Exception(f"Data Quality Failed: {dq_results['critical_failures']}")
    
    clean_records = dq_results['clean_records']
    
    print("\nüíæ Loading to ClickHouse Silver layer...")
    client = get_ch_client()
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ. –í–ê–ñ–ù–û: –¥–æ–±–∞–≤–∏—Ç—å unique_train_id –≤ —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    client.insert('train_delays', clean_records,
                  column_names=['timestamp', 'city', 'train_type', 'train_id',
                                'planned_departure', 'actual_departure',
                                'delay_in_min', 'is_cancelled', 'origin', 'destination', 
                                'unique_train_id']) # <--- –ù–û–í–û–ï –ü–û–õ–ï
    
    # –§–æ—Ä—Å–∏—Ä—É–µ–º —Å–ª–∏—è–Ω–∏–µ –≤ ClickHouse –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–∏—Å–∫–∞
    client.command("OPTIMIZE TABLE train_delays FINAL")
    
    log_pipeline_stage(DAG_ID, 'silver', 'SUCCESS', len(clean_records))
    return len(clean_records)


def ensure_silver_tables():
    """Create ClickHouse tables with UNIQUE ID support."""
    client = get_ch_client()
    
    # –ï—Å–ª–∏ —Å—Ö–µ–º–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (–¥–æ–±–∞–≤–∏–ª–∏ unique_id), –ª—É—á—à–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É (–¥–ª—è –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç–∞)
    # –í –ø—Ä–æ–¥–µ –º—ã –±—ã –¥–µ–ª–∞–ª–∏ ALTER TABLE ADD COLUMN
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ unique_train_id
        schema = client.query("DESCRIBE TABLE train_delays").result_rows
        columns = [row[0] for row in schema]
        if 'unique_train_id' not in columns:
            print("‚ö† Schema changed: Recreating table train_delays...")
            client.command("DROP TABLE IF EXISTS train_delays")
    except:
        pass
    
    # --- SILVER TABLE (ReplacingMergeTree) ---
    # ORDER BY —Ç–µ–ø–µ—Ä—å –≤–∫–ª—é—á–∞–µ—Ç unique_train_id –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
    client.command("""
        CREATE TABLE IF NOT EXISTS train_delays (
            timestamp DateTime,
            city String,
            train_type String,
            train_id String,              -- Human readable "ICE 101"
            planned_departure DateTime,
            actual_departure DateTime,
            delay_in_min Int32,
            is_cancelled UInt8,
            origin String,
            destination String,
            unique_train_id String        -- Internal DB ID "-2348234..."
        ) ENGINE = ReplacingMergeTree(timestamp)
        ORDER BY (city, unique_train_id, planned_departure)
        PARTITION BY toYYYYMM(planned_departure)
    """)
    
    # --- GOLD TABLE ---
    client.command("""
        CREATE TABLE IF NOT EXISTS daily_train_stats (
            stat_date Date,
            city String,
            train_type String,
            total_trains UInt32,
            delayed_trains UInt32,
            avg_delay Float32,
            max_delay Int32,
            created_at DateTime
        ) ENGINE = ReplacingMergeTree(created_at)
        ORDER BY (stat_date, city, train_type)
        PARTITION BY toYYYYMM(stat_date)
    """)
    
    print("‚úÖ ClickHouse tables ready (with unique_train_id)")


def run_silver_dq_checks(records: list) -> dict:
    results = {'critical_failures': [], 'clean_records': []}
    clean = []
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å, —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å 11 –ø–æ–ª–µ–π
    # r = [0:ts, 1:city, 2:type, 3:human_id, 4:planned, 5:actual, 6:delay, 7:canc, 8:orig, 9:dest, 10:UNIQUE_ID]
    
    for r in records:
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Å —É—á–µ—Ç–æ–º, —á—Ç–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å 10 (—Å—Ç–∞—Ä—ã–π –ø–∞—Ä—Å–µ—Ä) –∏–ª–∏ 11 (–Ω–æ–≤—ã–π)
        if len(r) == 11:
            timestamp, city, train_type, train_id, planned_dep, actual_dep, delay, cancelled, origin, dest, unique_id = r
        else:
             # Fallback
            timestamp, city, train_type, train_id, planned_dep, actual_dep, delay, cancelled, origin, dest = r
            # –ï—Å–ª–∏ unique_id –Ω–µ—Ç, –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π –∏–ª–∏ –∫–æ–ø–∏—é train_id, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞–ª–∞ –≤—Å—Ç–∞–≤–∫–∞
            r = list(r)
            r.append(train_id) 
            unique_id = train_id
            
        # DQ Checks
        if not train_id: continue
        if not planned_dep: continue
        if delay > 1000: continue 
        
        clean.append(r) # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å (–≤–æ–∑–º–æ–∂–Ω–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é)
        
    results['clean_records'] = clean
    return results


# =============================================================================
# GOLD LAYER: Aggregation
# =============================================================================

def gold_aggregate(**context):
    print("=" * 60)
    print("ü•á GOLD LAYER: Starting aggregation")
    print("=" * 60)
    
    client = get_ch_client()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º Gold —Å–ª–æ–π (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏)
    client.command("""
        ALTER TABLE daily_train_stats DELETE 
        WHERE stat_date >= toDate(now() - INTERVAL 1 DAY)
    """)
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–∑ Silver –≤ Gold
    # –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É ORDER BY –≤ Silver, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ (–ø–æ—Å–ª–µ FINAL)
    query = """
    INSERT INTO daily_train_stats
    SELECT
        toDate(planned_departure) as stat_date,
        city,
        train_type,
        count() as total_trains,
        countIf(delay_in_min > 5) as delayed_trains,
        avgIf(delay_in_min, delay_in_min > 5) as avg_delay,
        maxIf(delay_in_min, delay_in_min > 5) as max_delay,
        now() as created_at
    FROM train_delays FINAL
    WHERE planned_departure >= toStartOfDay(now() - INTERVAL 1 DAY)
    AND planned_departure <= now()  -- –¢–æ–ª—å–∫–æ —Å–æ—Å—Ç–æ—è–≤—à–∏–µ—Å—è –ø–æ–µ–∑–¥–∞!
    GROUP BY stat_date, city, train_type
    HAVING total_trains > 0
    """
    
    client.command(query)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ª–æ–≥–∞
    count = client.query("SELECT count() FROM daily_train_stats WHERE stat_date = toDate(now())").result_rows[0][0]
    log_pipeline_stage(DAG_ID, 'gold', 'SUCCESS', count)


# =============================================================================
# DAG DEFINITION
# =============================================================================

with DAG(
    dag_id=DAG_ID,
    default_args=default_args,
    description='Deutsche Bahn train delays: Bronze -> Silver -> Gold pipeline',
    schedule_interval='*/30 * * * *',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['deutsche-bahn', 'medallion', 'trains'],
) as dag:
    
    bronze = PythonOperator(task_id='bronze_extract', python_callable=bronze_extract, provide_context=True)
    silver = PythonOperator(task_id='silver_transform', python_callable=silver_transform, provide_context=True)
    gold = PythonOperator(task_id='gold_aggregate', python_callable=gold_aggregate, provide_context=True)
    
    bronze >> silver >> gold